{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7f9ea4-ebc7-4541-af5d-7c8cbc1b33a9",
   "metadata": {},
   "source": [
    "# Capstone Project: Topic Modelling of Academic Journals (Model-Based Systems Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c83ece-a75d-420a-8d38-cb74fd6d54ed",
   "metadata": {},
   "source": [
    "# 02: Preprocessing and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a31e9-0188-4cca-a174-1b3542a4e068",
   "metadata": {},
   "source": [
    "In this notebook, we will perform the following actions:\n",
    "1. Data preprocessing\n",
    "2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdff992-4c67-4de1-b296-a6d81278244b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a526d9e3-e212-4099-b622-5ef395e395e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "\n",
    "# Set all columns and rows to be displayed\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1ee27f-4a41-4c4b-95d7-879bfa613da8",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a1487f35-c44c-4e79-8bf1-0a2ee1033bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import journals data\n",
    "journals = pd.read_csv('../data/journals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7950b426-637b-4673-b7c4-b5e685ffe4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model-based Design Process for the Early Phase...</td>\n",
       "      <td>This paper presents an approach for a model-ba...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Based Systems Engineering using VHDL-AMS</td>\n",
       "      <td>The purpose of this paper is to contribute to ...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Code Generation Approach Supporting Complex Sy...</td>\n",
       "      <td>Code generation is an effective way to drive t...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model based systems engineering as enabler for...</td>\n",
       "      <td>Product complexity is steadily increasing, cus...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Electric Drive Vehicle Development and Evaluat...</td>\n",
       "      <td>To reduce development time and introduce techn...</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Model-based Design Process for the Early Phase...   \n",
       "1     Model Based Systems Engineering using VHDL-AMS   \n",
       "2  Code Generation Approach Supporting Complex Sy...   \n",
       "3  Model based systems engineering as enabler for...   \n",
       "4  Electric Drive Vehicle Development and Evaluat...   \n",
       "\n",
       "                                            abstract  year  \n",
       "0  This paper presents an approach for a model-ba...  2017  \n",
       "1  The purpose of this paper is to contribute to ...  2013  \n",
       "2  Code generation is an effective way to drive t...  2022  \n",
       "3  Product complexity is steadily increasing, cus...  2021  \n",
       "4  To reduce development time and introduce techn...  2014  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the dataframe\n",
    "journals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1c46a213-a7e6-4e86-8dfe-96438046a066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the data\n",
    "journals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf991526-a904-4ff2-b438-ea197ff2b97a",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ad196646-9267-4d2e-8568-cfe1832e5ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'abstract', 'year'], dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the columns in the dataframe\n",
    "journals.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba75dc2a-3026-4154-b628-b60e116f9b2f",
   "metadata": {},
   "source": [
    "Columns in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707024a1-546a-40f2-b863-88e98dff4043",
   "metadata": {},
   "source": [
    "|Column Name | Use of Column|\n",
    "|------------|--------------|\n",
    "|title| Title of the academic journal. Through topic modelling, each title will be assigned to a topic for quick search later on|\n",
    "|abstract| Abstract of each academic journal. This data will be preprocessed and used as the dataset for the unsupervised learning to identify topics|\n",
    "|year| Year that the academic journal was published. This will be used to identify shifts in trends between the topics over the years|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418cdbd2-8503-4d5a-a8ac-e2d96ed2a846",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ef74d-b346-407e-847d-9b897fb560aa",
   "metadata": {},
   "source": [
    "In this section, we will process the text data in the abstract column by cleaning the text, tokenizing and lemmatizing them. A description in more detail is provided below.\n",
    "* Cleaning the text to remove special characters\n",
    "* Tokenizing (converts sentences into individual words, and by using ngrams, we can also form tokens with multiple words to give better context)\n",
    "* Lemmatization (converts different words with the same meaning/intent into the same word)\n",
    "* Stop word removal (stop words are filler words that do not provide any context and just assist with sentence structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df770c-3942-43c8-b59b-1f505f573206",
   "metadata": {},
   "source": [
    "### Function Defintion for Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0559659f-8096-4abd-bf23-a5961c64125b",
   "metadata": {},
   "source": [
    "The below function will be used to preprocess the text data by perform the functions listed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4caa542a-b050-49ec-9278-4265d95c5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    # Remove 's\n",
    "    text = re.sub(r\"'s\", '', text)\n",
    "    \n",
    "    # Remove n't (example don't)\n",
    "    text = re.sub(r\"n't\", '', text)\n",
    "    \n",
    "    # Remove 'm (example I'm)\n",
    "    text = re.sub(r\"'m\", '', text)\n",
    "    \n",
    "    # Remove 'd (e.g. I'd)\n",
    "    text = re.sub(r\"'d\", '', text)\n",
    "    \n",
    "    # Remove 're (example They're)\n",
    "    text = re.sub(r\"'re\", '', text)\n",
    "    \n",
    "    # Remove 've (example They've)\n",
    "    text = re.sub(r\"'ve\", \" have\", text)\n",
    "    \n",
    "    # Remove 'll (example We'll)\n",
    "    text = re.sub(r\"'ll\", '', text)\n",
    "    \n",
    "    # Remove URL links\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Change all text to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove the word abstract as it was included as the first word in one of the dataset\n",
    "    text = re.sub(r\"abstract\", '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(i) for i in text]\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = [token for token in text if token not in stopwords.words('english')]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88fcc31-8af6-4d30-8ba4-6a103a3d63a7",
   "metadata": {},
   "source": [
    "### Preprocess the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8a500-4f80-44a5-b8ab-ad673389bc54",
   "metadata": {},
   "source": [
    "Here, we will apply the preprocess_text function to clean and tokenize our text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "97b88319-77ed-45ee-8b91-5d05c3fa34f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.1 s, sys: 9.16 s, total: 28.2 s\n",
      "Wall time: 33.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Place the proprocessed data as a new column called tokens\n",
    "journals['tokens'] = journals['abstract'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "be26a844-8297-4cae-8b87-cdb59902b3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [paper, present, approach, model-based, planni...\n",
       "1    [purpose, paper, contribute, definition, model...\n",
       "2    [code, generation, effective, way, drive, comp...\n",
       "3    [product, complexity, steadily, increasing, ,,...\n",
       "4    [reduce, development, time, introduce, technol...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the tokens\n",
    "journals['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5442e40b-9a34-454b-9e1b-eff55a669dc1",
   "metadata": {},
   "source": [
    "### Vectorize the words for EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f29e9-fe2d-479b-9702-e59ee203c012",
   "metadata": {},
   "source": [
    "We will use CountVectorizer to vectorize our words, to enable EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7c4993b2-7cb4-414f-9f4f-a33cfa20f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer with ngrams 1 for word frequency analysis\n",
    "cvec_journals_1 = CountVectorizer(lowercase=False, ngram_range=(1,1))\n",
    "\n",
    "# Instantiate a CountVectorizer with ngrams 2 for bigram analysis\n",
    "cvec_journals_2 = CountVectorizer(lowercase=False, ngram_range=(2,2))\n",
    "\n",
    "# Instantiate a CountVectorizer with ngrams 3 for trigram analysis\n",
    "cvec_journals_3 = CountVectorizer(lowercase=False, ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fbb5259e-df3a-4d41-8fd0-0965e6077c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tokenized words so that we can vectorize them\n",
    "journals['tokens'] = [\" \".join(post) for post in journals['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "554f7060-a38f-42bf-abe9-a65c2d937f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the three vectorizers, transform the data and export them into a dataframe\n",
    "\n",
    "# Unigrams\n",
    "cvec_journals_1.fit(journals['tokens'])\n",
    "journals_unigrams = cvec_journals_1.transform(journals['tokens'])\n",
    "journals_unigrams = pd.DataFrame(journals_unigrams.todense(), \n",
    "                                 columns=cvec_journals_1.get_feature_names_out())\n",
    "\n",
    "# Bigrams\n",
    "cvec_journals_2.fit(journals['tokens'])\n",
    "journals_bigrams = cvec_journals_2.transform(journals['tokens'])\n",
    "journals_bigrams = pd.DataFrame(journals_bigrams.todense(), \n",
    "                                 columns=cvec_journals_2.get_feature_names_out())\n",
    "\n",
    "# Trigrams\n",
    "cvec_journals_3.fit(journals['tokens'])\n",
    "journals_trigrams = cvec_journals_3.transform(journals['tokens'])\n",
    "journals_trigrams = pd.DataFrame(journals_trigrams.todense(), \n",
    "                                 columns=cvec_journals_3.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85681fc-9819-468d-8250-2be722322c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0abbc0-f320-419b-b569-a024f92e5447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84082ba-ede2-492c-b997-8cfa7a71e8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17841393-be0d-4cc1-94df-e0d2753f5e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['rows'] = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "405d2e00-f69b-4d6e-9df5-47c4bdfd7f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['articles'] = [\"i shall be having's a good times\", \n",
    "                   \"testings's they've, hadn't they're, and they'll i'd and shouldn't\", \n",
    "                    \"they've\",\n",
    "                   \"abstract\",\n",
    "                   \"123\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1d62dc99-3bb8-4e22-b484-2d9468b2541e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>articles</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>i shall be having's a good times</td>\n",
       "      <td>[shall, good, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>testings's they've, hadn't they're, and they'l...</td>\n",
       "      <td>[testing, ,, ,]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>they've</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>abstract</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>123</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rows                                           articles  \\\n",
       "0     1                   i shall be having's a good times   \n",
       "1     2  testings's they've, hadn't they're, and they'l...   \n",
       "2     3                                            they've   \n",
       "3     4                                           abstract   \n",
       "4     5                                                123   \n",
       "\n",
       "               cleaned  \n",
       "0  [shall, good, time]  \n",
       "1      [testing, ,, ,]  \n",
       "2                   []  \n",
       "3                   []  \n",
       "4                   []  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e371fd1-b291-4099-bfb9-d369edf12f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['cleaned'] = test['articles'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778816b-a0cb-41d6-8710-069e15c97521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>articles</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>i shall be having's a good times</td>\n",
       "      <td>[shall, good, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>testings's they've, hadn't they're, and they'l...</td>\n",
       "      <td>[testing, ,, ,]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>they've</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>i'm</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>123</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rows                                           articles  \\\n",
       "0     1                   i shall be having's a good times   \n",
       "1     2  testings's they've, hadn't they're, and they'l...   \n",
       "2     3                                            they've   \n",
       "3     4                                                i'm   \n",
       "4     5                                                123   \n",
       "\n",
       "               cleaned  \n",
       "0  [shall, good, time]  \n",
       "1      [testing, ,, ,]  \n",
       "2                   []  \n",
       "3                   []  \n",
       "4                   []  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8cdd0a-0d16-4af6-b4f4-9f254eb2878b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76436db6-40c9-44ef-bd90-4574216226ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c5fb6-f930-48c8-8ee4-b67ce0be6d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c9d14-1253-4383-8395-a9e5334fbd04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi-sg",
   "language": "python",
   "name": "dsi-sg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
