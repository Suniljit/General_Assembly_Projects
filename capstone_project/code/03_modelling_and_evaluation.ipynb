{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7f9ea4-ebc7-4541-af5d-7c8cbc1b33a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Capstone Project: Topic Modelling of Academic Journals (Model-Based Systems Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c83ece-a75d-420a-8d38-cb74fd6d54ed",
   "metadata": {},
   "source": [
    "# 03: Modelling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a31e9-0188-4cca-a174-1b3542a4e068",
   "metadata": {},
   "source": [
    "In this notebook, we will perform the following actions:\n",
    "1. Topic Modelling\n",
    "2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdff992-4c67-4de1-b296-a6d81278244b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a526d9e3-e212-4099-b622-5ef395e395e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/dsi-sg/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/usr/local/Caskroom/miniconda/base/envs/dsi-sg/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/usr/local/Caskroom/miniconda/base/envs/dsi-sg/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/usr/local/Caskroom/miniconda/base/envs/dsi-sg/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from bertopic import BERTopic\n",
    "#from gensim.models.ldamodel import LdaModel\n",
    "#from gensim.corpora.dictionary import Dictionary\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Set all columns and rows to be displayed\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52287f4-adc2-46f5-bccf-d637abf71afb",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9851fb-181e-4cb5-aac6-00194d1f24cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data for modelling\n",
    "journals = pd.read_csv('../data/journals_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ea8fe-636b-4694-9c22-894195087725",
   "metadata": {},
   "source": [
    "## Final Data Preprocessing using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83e72e-2214-4699-8b3f-deadfe2cf219",
   "metadata": {},
   "source": [
    "In this section, we will perform our final data preprocessing using TF-IDF (Term Frequency - Inverse Document Frequency). TF-IDF is used as it takes into account how often a word appears in the whole corpus. This helps to penalize common words that appear across every document, which is not informative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b32fd35f-3c13-405b-870a-c213f1975eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a TF-IDF Vectorizer\n",
    "tvec_journals = TfidfVectorizer(lowercase=False, ngram_range=(1,3))\n",
    "\n",
    "# Fit and transform the text data to prepare for topic modelling\n",
    "journals_corpus = tvec_journals.fit_transform(journals['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25b21c-c555-4898-823c-a9337e995a5b",
   "metadata": {},
   "source": [
    "## Topic Modelling using Latent Dirichlet Allocation (LDA) - sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0c9dc-f466-43f3-852b-d1cf3e0c79e0",
   "metadata": {},
   "source": [
    "Here, we will perform topic modelling using LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13bc0802-f065-4e91-bdc0-49ed6edae15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=5, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=5, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=5,\n",
    "                                     random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "lda_model.fit(journals_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27af1838-7881-4385-8069-41279f8fcabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "based requirement model tool model based analysis method industry support complexity simulation research lifecycle concept language\n",
      "\n",
      "Topic #1:\n",
      "requirement based tool model analysis model based methodology framework method domain complex language human simulation study\n",
      "\n",
      "Topic #2:\n",
      "language based model tool model based method new analysis project information framework requirement support study case\n",
      "\n",
      "Topic #3:\n",
      "requirement based simulation model method model based tool language analysis data safety domain framework verification environment\n",
      "\n",
      "Topic #4:\n",
      "based simulation digital requirement model analysis model based tool safety information method language project data twin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the top words for each topic\n",
    "feature_names = tvec_journals.get_feature_names_out()\n",
    "n_top_words = 15\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(\"Topic #%d:\" %topic_idx)\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words -1:-1]]))\n",
    "    print()\n",
    "    \n",
    "# Extract the topic distribution for each journal\n",
    "journal_topic_dist = lda_model.transform(journals_corpus)\n",
    "\n",
    "# Create a dataframe to store the journal topics probability distribution\n",
    "df_journal_topic_dist = pd.DataFrame(journal_topic_dist, columns=['Topic 0', 'Topic 1', 'Topic 2', 'Topic 3', 'Topic 4'])\n",
    "\n",
    "# Add in a column with the topic generated \n",
    "df_journal_topic_dist['topic_generated'] = journal_topic_dist.argmax(axis=1)\n",
    "\n",
    "# Add in the title of the journal\n",
    "df_journal_topic_dist['title'] = journals['title']\n",
    "\n",
    "# Add in the publication year of each journal\n",
    "df_journal_topic_dist['year'] = journals['year'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a73c916-3e65-4cf7-8fee-265b16c8ea97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>topic_generated</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015411</td>\n",
       "      <td>0.015428</td>\n",
       "      <td>0.938266</td>\n",
       "      <td>0.015417</td>\n",
       "      <td>0.015478</td>\n",
       "      <td>2</td>\n",
       "      <td>Model-based Design Process for the Early Phase...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.014271</td>\n",
       "      <td>0.014305</td>\n",
       "      <td>0.942570</td>\n",
       "      <td>0.014555</td>\n",
       "      <td>3</td>\n",
       "      <td>Model Based Systems Engineering using VHDL-AMS</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013795</td>\n",
       "      <td>0.013762</td>\n",
       "      <td>0.013797</td>\n",
       "      <td>0.944830</td>\n",
       "      <td>0.013816</td>\n",
       "      <td>3</td>\n",
       "      <td>Code Generation Approach Supporting Complex Sy...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012287</td>\n",
       "      <td>0.012286</td>\n",
       "      <td>0.012273</td>\n",
       "      <td>0.012290</td>\n",
       "      <td>0.950865</td>\n",
       "      <td>4</td>\n",
       "      <td>Model based systems engineering as enabler for...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014573</td>\n",
       "      <td>0.014557</td>\n",
       "      <td>0.014590</td>\n",
       "      <td>0.014566</td>\n",
       "      <td>0.941714</td>\n",
       "      <td>4</td>\n",
       "      <td>Electric Drive Vehicle Development and Evaluat...</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic 0   Topic 1   Topic 2   Topic 3   Topic 4  topic_generated  \\\n",
       "0  0.015411  0.015428  0.938266  0.015417  0.015478                2   \n",
       "1  0.014299  0.014271  0.014305  0.942570  0.014555                3   \n",
       "2  0.013795  0.013762  0.013797  0.944830  0.013816                3   \n",
       "3  0.012287  0.012286  0.012273  0.012290  0.950865                4   \n",
       "4  0.014573  0.014557  0.014590  0.014566  0.941714                4   \n",
       "\n",
       "                                               title  year  \n",
       "0  Model-based Design Process for the Early Phase...  2017  \n",
       "1     Model Based Systems Engineering using VHDL-AMS  2013  \n",
       "2  Code Generation Approach Supporting Complex Sy...  2022  \n",
       "3  Model based systems engineering as enabler for...  2021  \n",
       "4  Electric Drive Vehicle Development and Evaluat...  2014  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_journal_topic_dist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f1bc39-3895-4747-9db4-8828af55ff4b",
   "metadata": {},
   "source": [
    "## Topic Modeling using BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00613e79-3d0a-4fae-9063-3dc6f081e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a BERTopic model\n",
    "bertopic_model = BERTopic()\n",
    "\n",
    "# Fit and transform the model to the corpus\n",
    "topics, _ = bertopic_model.fit_transform(journals['tokens'])\n",
    "\n",
    "# Print the top words for each topic\n",
    "for topic_id in range(max(topics)):\n",
    "    words = bertopic_model.get_topic(topic_id)\n",
    "    print(f\"Topic {topic_id}: {' | '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d971f18-8d71-4e0b-8cf7-f99e77cdfbad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73008b3-b632-4801-91ee-b45aa4b7c3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28251df8-fbc1-4f03-9202-dc40b9b51c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60441978-265a-41c1-ac0d-a29913de46d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d730bd-9339-4ea4-aa54-52caced19f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81021b-d8b7-49f1-9df4-2aa068d88b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc1882d-093c-4e75-87d5-fe891b44764c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92cb6f-04d5-4dbe-b8ca-1fbc4a95cce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8d076-35fa-41ae-b3dc-23f8dfafe5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffde2e-c633-4ad2-82ca-6e8355841bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230c53f-8e86-4451-9f6c-f9d6f38b648c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716ccba-35d4-46fc-9014-5aaff7bf68af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ec3e6-41f9-4456-a23f-dcc9d7bbf6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fee6a9-fe9b-4f53-a0bb-1f502e447c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69c7b0ae-0332-465c-bb00-a32b6de242ea",
   "metadata": {},
   "source": [
    "## Topic Modelling using Latent Dirichlet Allocation (LDA) - gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2155a-6149-4ea9-b464-7ab73f9bd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a TF-IDF Vectorizer\n",
    "tvec_2 = TfidfVectorizer(lowercase=False, ngram_range=(1,3))\n",
    "\n",
    "# Fit and transform the text data to prepare for topic modelling\n",
    "tfidf_matrix = tvec_2.fit_transform(journals['tokens'])\n",
    "\n",
    "# Create a gensim dictionary \n",
    "dictionary = Dictionary([abstract.split() for abstract in journals['tokens']])\n",
    "\n",
    "# Create the gensim corpus\n",
    "corpus = [dictionary.doc2bow(abstract.split()) for abstract in journals['tokens']]\n",
    "\n",
    "# Create the gensim LDA model\n",
    "lda_model_2 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=10)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model_2.print_topics(num_words=15)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50803842-da48-4418-afe6-b11d9c2da630",
   "metadata": {},
   "source": [
    "stopped at trying to get gensim's LDA model to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc5f38-dcda-4395-a609-d7f3d92c9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_2.show_topics(formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8651475-e296-43d2-afa3-dd4fdcc9219c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e49b3c-97c6-4bac-889e-7e026b5943f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbda441-4d4a-4536-9a96-b12accbf823c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a0068-7639-4373-868f-48b21b5e4b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad2917-6e46-41b5-bb94-056f755719eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faab46c-0334-42fb-9dff-5ea0d814e77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290f4f1-c9e0-4875-9d0d-0a0d64b22418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7e4ed-1f74-4167-81ca-1a91744ffc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d6303b-122e-4078-af73-75637e0b85ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a1ee27f-4a41-4c4b-95d7-879bfa613da8",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1487f35-c44c-4e79-8bf1-0a2ee1033bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import journals data\n",
    "journals = pd.read_csv('../data/journals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950b426-637b-4673-b7c4-b5e685ffe4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the dataframe\n",
    "journals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46a213-a7e6-4e86-8dfe-96438046a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the data\n",
    "journals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf991526-a904-4ff2-b438-ea197ff2b97a",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad196646-9267-4d2e-8568-cfe1832e5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in the dataframe\n",
    "journals.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba75dc2a-3026-4154-b628-b60e116f9b2f",
   "metadata": {},
   "source": [
    "Columns in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707024a1-546a-40f2-b863-88e98dff4043",
   "metadata": {},
   "source": [
    "|Column Name | Use of Column|\n",
    "|------------|--------------|\n",
    "|title| Title of the academic journal. Through topic modelling, each title will be assigned to a topic for quick search later on|\n",
    "|abstract| Abstract of each academic journal. This data will be preprocessed and used as the dataset for the unsupervised learning to identify topics|\n",
    "|year| Year that the academic journal was published. This will be used to identify shifts in trends between the topics over the years|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418cdbd2-8503-4d5a-a8ac-e2d96ed2a846",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ef74d-b346-407e-847d-9b897fb560aa",
   "metadata": {},
   "source": [
    "In this section, we will process the text data in the abstract column by cleaning the text, tokenizing and lemmatizing them. A description in more detail is provided below.\n",
    "* Cleaning the text to remove special characters\n",
    "* Tokenizing (converts sentences into individual words, and by using ngrams, we can also form tokens with multiple words to give better context)\n",
    "* Lemmatization (converts different words with the same meaning/intent into the same word)\n",
    "* Stop word removal (stop words are filler words that do not provide any context and just assist with sentence structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b5cc0-c617-4d46-8ec9-c8c0520e4082",
   "metadata": {},
   "source": [
    "### Definition of Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b3d29-9d9b-4b19-9fe2-f99169db7273",
   "metadata": {},
   "source": [
    "We will assign the stopwords from NLTK to a list called stop_words. This is so that the list can be further expanded later on when looking at the word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ea59f-3fde-47ca-8429-d5b3146a4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df770c-3942-43c8-b59b-1f505f573206",
   "metadata": {},
   "source": [
    "### Function Defintion for Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0559659f-8096-4abd-bf23-a5961c64125b",
   "metadata": {},
   "source": [
    "The below function will be used to preprocess the text data by perform the functions listed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa542a-b050-49ec-9278-4265d95c5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    # Remove 's\n",
    "    text = re.sub(r\"'s\", '', text)\n",
    "    \n",
    "    # Remove n't (example don't)\n",
    "    text = re.sub(r\"n't\", '', text)\n",
    "    \n",
    "    # Remove 'm (example I'm)\n",
    "    text = re.sub(r\"'m\", '', text)\n",
    "    \n",
    "    # Remove 'd (e.g. I'd)\n",
    "    text = re.sub(r\"'d\", '', text)\n",
    "    \n",
    "    # Remove 're (example They're)\n",
    "    text = re.sub(r\"'re\", '', text)\n",
    "    \n",
    "    # Remove 've (example They've)\n",
    "    text = re.sub(r\"'ve\", \" have\", text)\n",
    "    \n",
    "    # Remove 'll (example We'll)\n",
    "    text = re.sub(r\"'ll\", '', text)\n",
    "    \n",
    "    # Remove URL links\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Change all text to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove the word abstract as it was included as the first word in one of the dataset\n",
    "    text = re.sub(r\"abstract\", '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize the text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(i) for i in text]\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = [token for token in text if token not in stop_words]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88fcc31-8af6-4d30-8ba4-6a103a3d63a7",
   "metadata": {},
   "source": [
    "### Preprocess the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8a500-4f80-44a5-b8ab-ad673389bc54",
   "metadata": {},
   "source": [
    "Here, we will apply the preprocess_text function to clean and tokenize our text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b88319-77ed-45ee-8b91-5d05c3fa34f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Place the proprocessed data as a new column called tokens\n",
    "journals['tokens'] = journals['abstract'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26a844-8297-4cae-8b87-cdb59902b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the tokens\n",
    "journals['tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5442e40b-9a34-454b-9e1b-eff55a669dc1",
   "metadata": {},
   "source": [
    "### Vectorize the words for EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f29e9-fe2d-479b-9702-e59ee203c012",
   "metadata": {},
   "source": [
    "We will use CountVectorizer to vectorize our words, to enable EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5259e-df3a-4d41-8fd0-0965e6077c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tokenized words so that we can vectorize them\n",
    "journals['tokens'] = [\" \".join(post) for post in journals['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4993b2-7cb4-414f-9f4f-a33cfa20f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer with ngrams 1 for word frequency analysis\n",
    "cvec_journals_1 = CountVectorizer(lowercase=False, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f7060-a38f-42bf-abe9-a65c2d937f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Count Vectorizer, transform the data and export them into a dataframe\n",
    "\n",
    "# Unigrams\n",
    "cvec_journals_1.fit(journals['tokens'])\n",
    "journals_unigrams = cvec_journals_1.transform(journals['tokens'])\n",
    "journals_unigrams = pd.DataFrame(journals_unigrams.todense(), \n",
    "                                 columns=cvec_journals_1.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3aa655-2a7d-492e-8527-ba3f5e1189fa",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0031900-9f12-4b1f-948f-bf7d0436e600",
   "metadata": {},
   "source": [
    "In this section, we'll conduct EDA to look at the distribution of the text as well as most frequently occuring words. Furthermore, based on the EDA, we will further clean the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4bd723-9d69-4835-b261-e2097ce182bb",
   "metadata": {},
   "source": [
    "#### Document Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c3ddc-61dd-4650-b2fa-a41dadfcb87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dataframe to store the additional EDA data\n",
    "eda_journals = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12556028-708a-49d5-be56-f69989cffdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the word count for the abstracts\n",
    "eda_journals['word_count'] = journals['tokens'].map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d363fd0e-2529-435d-8d05-b9f586a936e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram plot to view the distribution of the word count in the abstracts\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(eda_journals['word_count'], kde=False, bins=20, label=\"Word Count\", \n",
    "             color=\"blue\", alpha = 0.7)\n",
    "\n",
    "plt.title(\"Distribution of Word Count in the Abstracts\")\n",
    "plt.xlabel(\"Word Count in the Abstracts\")\n",
    "plt.ylabel(\"Frequency of Count\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5d3d4-750e-4d16-80c7-b5f51d65e0af",
   "metadata": {},
   "source": [
    "#### Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03f851-6867-4faa-b9ee-02f7f3535245",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the 100 most frequently occuring unigrams in the abstracts\n",
    "plt.figure(figsize=(16,25))\n",
    "top_100_words = journals_unigrams.sum().sort_values(ascending=False).head(100)\n",
    "top_100_words.sort_values(ascending=True).plot(kind='barh');\n",
    "plt.title('100 Most Frequently Occurring Words in the Abstracts')\n",
    "plt.ylabel('Words')\n",
    "plt.xlabel('Number of Occurences');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec810ba-51fc-4157-a14a-e3aad6748c3b",
   "metadata": {},
   "source": [
    "From the above distribution, we can identify various words to add into our stopwords: $using, used, ha, use, also, within, however, well, wa, example$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b306c-e3ec-4d67-9d11-6415ef4d2498",
   "metadata": {},
   "source": [
    "#### Remove the Additional Stopwords Identified Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e933f-ba78-4872-8c8d-3b6746db1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of high frequency words that are identified as stopwords\n",
    "additional_stop_words = ['using', 'used', 'ha', 'use', 'also', 'within', \n",
    "                         'however', 'well', 'wa', 'example']\n",
    "\n",
    "# Add the additional stop words to the original stop word list\n",
    "stop_words.extend(additional_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e7fb1-dadf-4abb-88a0-fc67cd38341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Preprocess the data again\n",
    "journals['tokens'] = journals['abstract'].apply(preprocess_text)\n",
    "\n",
    "# Join the tokenized words so that we can vectorize them\n",
    "journals['tokens'] = [\" \".join(post) for post in journals['tokens']]\n",
    "\n",
    "# Instantiate a CountVectorizer with ngrams 1 for word frequency analysis\n",
    "cvec_journals_1 = CountVectorizer(lowercase=False, ngram_range=(1,1))\n",
    "\n",
    "# Instantiate a CountVectorizer with ngrams 2 for bigram analysis\n",
    "cvec_journals_2 = CountVectorizer(lowercase=False, ngram_range=(2,2))\n",
    "\n",
    "# Instantiate a CountVectorizer with ngrams 3 for trigram analysis\n",
    "cvec_journals_3 = CountVectorizer(lowercase=False, ngram_range=(3,3))\n",
    "\n",
    "\n",
    "# Fit the three vectorizers, transform the data and export them into a dataframe\n",
    "\n",
    "# Unigrams\n",
    "cvec_journals_1.fit(journals['tokens'])\n",
    "journals_unigrams = cvec_journals_1.transform(journals['tokens'])\n",
    "journals_unigrams = pd.DataFrame(journals_unigrams.todense(), \n",
    "                                 columns=cvec_journals_1.get_feature_names_out())\n",
    "\n",
    "# Bigrams\n",
    "cvec_journals_2.fit(journals['tokens'])\n",
    "journals_bigrams = cvec_journals_2.transform(journals['tokens'])\n",
    "journals_bigrams = pd.DataFrame(journals_bigrams.todense(), \n",
    "                                 columns=cvec_journals_2.get_feature_names_out())\n",
    "\n",
    "# Trigrams\n",
    "cvec_journals_3.fit(journals['tokens'])\n",
    "journals_trigrams = cvec_journals_3.transform(journals['tokens'])\n",
    "journals_trigrams = pd.DataFrame(journals_trigrams.todense(), \n",
    "                                 columns=cvec_journals_3.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f275b89-cf53-4a7d-9723-4c3a750ba211",
   "metadata": {},
   "source": [
    "#### Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef51249-9d02-4e71-abfc-e91a5b316ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 25 most frequently occuring unigrams in the abstracts\n",
    "plt.figure(figsize=(16,12))\n",
    "top_25_unigrams = journals_unigrams.sum().sort_values(ascending=False).head(25)\n",
    "top_25_unigrams.sort_values(ascending=True).plot(kind='barh');\n",
    "plt.title('25 Most Frequently Occurring Unigrams in the Abstracts')\n",
    "plt.ylabel('Words')\n",
    "plt.xlabel('Number of Occurences');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac8591-d9fd-4b8a-84df-19843f93bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 25 most frequently occuring bigrams in the abstracts\n",
    "plt.figure(figsize=(16,12))\n",
    "top_25_bigrams = journals_bigrams.sum().sort_values(ascending=False).head(25)\n",
    "top_25_bigrams.sort_values(ascending=True).plot(kind='barh');\n",
    "plt.title('25 Most Frequently Occurring Bigrams in the Abstracts')\n",
    "plt.ylabel('Words')\n",
    "plt.xlabel('Number of Occurences');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4007193-754d-42c4-983f-2be31ef03f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 25 most frequently occuring trigrams in the abstracts\n",
    "plt.figure(figsize=(16,12))\n",
    "top_25_trigrams = journals_trigrams.sum().sort_values(ascending=False).head(25)\n",
    "top_25_trigrams.sort_values(ascending=True).plot(kind='barh');\n",
    "plt.title('25 Most Frequently Occurring Trigrams in the Abstracts')\n",
    "plt.ylabel('Words')\n",
    "plt.xlabel('Number of Occurences');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d52bbd-f341-49fb-977b-b94519629266",
   "metadata": {},
   "source": [
    "#### Word Clouds of the Unigrams, Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc446e4-9faa-48e4-8ac3-756b438621af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequencies of the words in the Unigrams, Bigrams and Trigrams\n",
    "unigrams_count = journals_unigrams.sum().sort_values(ascending=False)\n",
    "bigrams_count = journals_bigrams.sum().sort_values(ascending=False)\n",
    "trigrams_count = journals_trigrams.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0abbc0-f320-419b-b569-a024f92e5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word cloud for the unigrams\n",
    "wordcloud_unigrams = WordCloud(max_words=100, width=1000, height=1000, \n",
    "                             background_color='white').generate_from_frequencies(unigrams_count)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.imshow(wordcloud_unigrams)\n",
    "plt.axis('off')\n",
    "plt.title('Unigrams', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e9f03-6cf7-4dff-bebf-0e369f091ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word cloud for the bigrams\n",
    "wordcloud_bigrams = WordCloud(max_words=100, width=1000, height=1000, \n",
    "                             background_color='white').generate_from_frequencies(bigrams_count)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.imshow(wordcloud_bigrams)\n",
    "plt.axis('off')\n",
    "plt.title('Bigrams', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b69c04-1feb-4d1e-9824-69ce5cdb4ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word cloud for the trigrams\n",
    "wordcloud_trigrams = WordCloud(max_words=100, width=1000, height=1000, \n",
    "                             background_color='white').generate_from_frequencies(trigrams_count)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.imshow(wordcloud_trigrams)\n",
    "plt.axis('off')\n",
    "plt.title('Trigrams', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b9fec-b1b0-431c-8e8a-ef8fea189632",
   "metadata": {},
   "source": [
    "## Final Data Preprocessing using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27007fbb-7445-49a7-a67c-e30cf73501f7",
   "metadata": {},
   "source": [
    "In this section, we will perform our final data preprocessing using TF-IDF (Term Frequency - Inverse Document Frequency). TF-IDF is used as it takes into account how often a word appears in the whole corpus. This helps to penalize common words that appear across every document, which is not informative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f8a0c-fa0e-4a86-a2d1-c3b9223c053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Preprocess the data again\n",
    "journals['tokens'] = journals['abstract'].apply(preprocess_text)\n",
    "\n",
    "# Join the tokenized words so that we can vectorize them\n",
    "journals['tokens'] = [\" \".join(post) for post in journals['tokens']]\n",
    "\n",
    "# Instantiate a TF-IDF Vectorizer\n",
    "tvec_journals = TfidfVectorizer(lowercase=False, ngram_range=(1,3))\n",
    "\n",
    "# Fit and transform the text data to prepare for topic modelling\n",
    "journals_corpus = tvec_journals.fit_transform(journals['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead80c83-f437-465c-8ec0-26975364bfcd",
   "metadata": {},
   "source": [
    "## Export the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be3c87a-a185-4f3c-bd19-396605f93574",
   "metadata": {},
   "source": [
    "Now that we have completed the data processing, let's export the data into another notebook to perform topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73baad8f-0d55-4bdb-84eb-fd0d6f9020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pickle to export the data\n",
    "pickle.dump(journals_corpus, open('../data/journals_corpus.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi-sg",
   "language": "python",
   "name": "dsi-sg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
